{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DIR = './data/target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "! echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Config / Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "lr = 0.00001\n",
    "gamma = 0.99\n",
    "epochs = 300\n",
    "epoch_to_record = 150\n",
    "log_interval = 1\n",
    "dropout_rate = 0.2\n",
    "\n",
    "num_classes = 50\n",
    "shift_cls = 0\n",
    "\n",
    "model_path = './checkpoints/ir-model-01.pt'\n",
    "\n",
    "seed = 319"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrDataset(Dataset):\n",
    "    def __init__(self, file_name, root_dir, shift_cls, num_classes, transform=None):\n",
    "        self.df = pd.read_pickle(root_dir + file_name)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        xs = self.df.iloc[idx]['spectrum'].reshape(1, -1)\n",
    "        w_cols = [i for i in range(num_classes + 3 + shift_cls, num_classes + 3 + shift_cls + num_classes, 1)]\n",
    "        ws = self.df.iloc[idx][w_cols].astype('float').values.reshape(-1)\n",
    "        y_cols = [i for i in range(2 + shift_cls, 2 + shift_cls + num_classes, 1)]\n",
    "        ys = self.df.iloc[idx][y_cols].astype('float').values.reshape(-1)\n",
    "        xs = torch.from_numpy(xs).float()\n",
    "        ys = torch.from_numpy(ys).float()\n",
    "        ws = torch.from_numpy(ws).float()\n",
    "        sample = { \n",
    "            'xs': xs,\n",
    "            'ws': ws, \n",
    "            'ys': ys \n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IrDataset(\n",
    "    file_name='/df_train.pk',\n",
    "    root_dir=TARGET_DIR,\n",
    "    shift_cls=shift_cls,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "valid_dataset = IrDataset(\n",
    "    file_name='/df_valid.pk',\n",
    "    root_dir=TARGET_DIR,\n",
    "    shift_cls=shift_cls,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "test_dataset = IrDataset(\n",
    "    file_name='/df_test.pk',\n",
    "    root_dir=TARGET_DIR,\n",
    "    shift_cls=shift_cls,\n",
    "    num_classes=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=len(valid_dataset), shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([32, 1, 3400]) torch.Size([32, 50]) torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch_spl in enumerate(train_loader):\n",
    "    print(\n",
    "        batch_idx,\n",
    "        batch_spl['xs'].size(),\n",
    "        batch_spl['ws'].size(),\n",
    "        batch_spl['ys'].size()\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb35ece1f90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=2, kernel_size=11, stride=1, padding=5)\n",
    "        self.conv1_bn = nn.BatchNorm1d(2)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.conv2 = nn.Conv1d(in_channels=2, out_channels=4, kernel_size=11, stride=1, padding=5)\n",
    "        self.conv2_bn = nn.BatchNorm1d(4)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.conv3 = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=11, stride=1, padding=5)\n",
    "        self.conv3_bn = nn.BatchNorm1d(8)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc1 = nn.Linear(3400, 1000)\n",
    "        self.fc1_bn = nn.BatchNorm1d(1000)\n",
    "        self.dropout_fc1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(1000, 250)\n",
    "        self.fc2_bn = nn.BatchNorm1d(250)\n",
    "        self.dropout_fc2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(250, 64)\n",
    "        self.fc3_bn = nn.BatchNorm1d(64)\n",
    "        self.dropout_fc3 = nn.Dropout(dropout_rate)\n",
    "        self.fc4 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.conv1_bn(\n",
    "            F.relu(\n",
    "                self.conv1(x)\n",
    "            )\n",
    "        )\n",
    "        x = self.dropout1(z)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        z = self.conv2_bn(\n",
    "            F.relu(\n",
    "                self.conv2(x)\n",
    "            )\n",
    "        )\n",
    "        x = self.dropout2(z)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        z = self.conv3_bn(\n",
    "            F.relu(\n",
    "                self.conv3(x)\n",
    "            )\n",
    "        )\n",
    "        x = self.dropout3(z)\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = F.relu(self.fc1_bn(self.fc1(x)))\n",
    "        x = self.dropout_fc1(x)\n",
    "        x = F.relu(self.fc2_bn(self.fc2(x)))\n",
    "        x = self.dropout_fc2(x)\n",
    "        x = F.relu(self.fc3_bn(self.fc3(x)))\n",
    "        x = self.dropout_fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        output = torch.sigmoid(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "criterion = nn.BCELoss(reduction='none')\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train / Test processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = []\n",
    "\n",
    "def train_process(model, device, data_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    counter = 0.0\n",
    "    for batch_idx, data in enumerate(data_loader):\n",
    "        xs, ys, ws = data['xs'].to(device), data['ys'].to(device), data['ws'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(xs)\n",
    "        loss = criterion(output, ys)\n",
    "        loss = (loss * ws).mean()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        counter += 1.0\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    train_results.append((epoch, total_loss / counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_results = []\n",
    "min_verify_loss = 10000\n",
    "\n",
    "def verify_process(model, device, data_loader, criterion, epoch):\n",
    "    if epoch % log_interval == 0:\n",
    "        model.eval()\n",
    "\n",
    "        ans = []\n",
    "        with torch.no_grad():\n",
    "            for data in data_loader:\n",
    "                xs, ys, ws = data['xs'].to(device), data['ys'].to(device), data['ws'].to(device)\n",
    "                output = model(xs)\n",
    "                loss = criterion(output, ys)\n",
    "                loss = (loss * ws).mean()\n",
    "                \n",
    "                ans.append((output, ys))\n",
    "                \n",
    "            verify_results.append((epoch, loss.item()))\n",
    "            \n",
    "            global epochs\n",
    "            if epoch >= epoch_to_record:\n",
    "                global min_verify_loss\n",
    "                if min_verify_loss > loss:\n",
    "                    min_verify_loss = loss\n",
    "                    torch.save(model, model_path)\n",
    "                    print('Model Save! @ epoch {}, Loss {:.4f}'.format(epoch, loss))\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000; Train Loss: 0.73938; Validation Loss: 0.72275\n",
      "Epoch: 001; Train Loss: 0.71939; Validation Loss: 0.71193\n",
      "Epoch: 002; Train Loss: 0.70675; Validation Loss: 0.69987\n",
      "Epoch: 003; Train Loss: 0.69595; Validation Loss: 0.69456\n",
      "Epoch: 004; Train Loss: 0.68330; Validation Loss: 0.68883\n",
      "Epoch: 005; Train Loss: 0.67460; Validation Loss: 0.68201\n",
      "Epoch: 006; Train Loss: 0.66761; Validation Loss: 0.67522\n",
      "Epoch: 007; Train Loss: 0.66187; Validation Loss: 0.66844\n",
      "Epoch: 008; Train Loss: 0.64970; Validation Loss: 0.66081\n",
      "Epoch: 009; Train Loss: 0.64758; Validation Loss: 0.65496\n",
      "Epoch: 010; Train Loss: 0.64240; Validation Loss: 0.65001\n",
      "Epoch: 011; Train Loss: 0.63529; Validation Loss: 0.64642\n",
      "Epoch: 012; Train Loss: 0.62647; Validation Loss: 0.63528\n",
      "Epoch: 013; Train Loss: 0.62299; Validation Loss: 0.63197\n",
      "Epoch: 014; Train Loss: 0.61760; Validation Loss: 0.62879\n",
      "Epoch: 015; Train Loss: 0.61517; Validation Loss: 0.62334\n",
      "Epoch: 016; Train Loss: 0.60617; Validation Loss: 0.61583\n",
      "Epoch: 017; Train Loss: 0.60556; Validation Loss: 0.61014\n",
      "Epoch: 018; Train Loss: 0.59866; Validation Loss: 0.60696\n",
      "Epoch: 019; Train Loss: 0.59243; Validation Loss: 0.60045\n",
      "Epoch: 020; Train Loss: 0.58844; Validation Loss: 0.59883\n",
      "Epoch: 021; Train Loss: 0.58337; Validation Loss: 0.59489\n",
      "Epoch: 022; Train Loss: 0.57847; Validation Loss: 0.59172\n",
      "Epoch: 023; Train Loss: 0.57426; Validation Loss: 0.58392\n",
      "Epoch: 024; Train Loss: 0.57399; Validation Loss: 0.58113\n",
      "Epoch: 025; Train Loss: 0.56576; Validation Loss: 0.57591\n",
      "Epoch: 026; Train Loss: 0.56518; Validation Loss: 0.57540\n",
      "Epoch: 027; Train Loss: 0.56007; Validation Loss: 0.57054\n",
      "Epoch: 028; Train Loss: 0.55522; Validation Loss: 0.56439\n",
      "Epoch: 029; Train Loss: 0.55130; Validation Loss: 0.56101\n",
      "Epoch: 030; Train Loss: 0.54832; Validation Loss: 0.55789\n",
      "Epoch: 031; Train Loss: 0.54382; Validation Loss: 0.55713\n",
      "Epoch: 032; Train Loss: 0.54062; Validation Loss: 0.55462\n",
      "Epoch: 033; Train Loss: 0.53553; Validation Loss: 0.55353\n",
      "Epoch: 034; Train Loss: 0.53353; Validation Loss: 0.54446\n",
      "Epoch: 035; Train Loss: 0.52954; Validation Loss: 0.53852\n",
      "Epoch: 036; Train Loss: 0.52814; Validation Loss: 0.53642\n",
      "Epoch: 037; Train Loss: 0.52461; Validation Loss: 0.53190\n",
      "Epoch: 038; Train Loss: 0.52192; Validation Loss: 0.53109\n",
      "Epoch: 039; Train Loss: 0.51715; Validation Loss: 0.53079\n",
      "Epoch: 040; Train Loss: 0.51299; Validation Loss: 0.52436\n",
      "Epoch: 041; Train Loss: 0.50903; Validation Loss: 0.52134\n",
      "Epoch: 042; Train Loss: 0.50511; Validation Loss: 0.51621\n",
      "Epoch: 043; Train Loss: 0.50435; Validation Loss: 0.51083\n",
      "Epoch: 044; Train Loss: 0.50090; Validation Loss: 0.50983\n",
      "Epoch: 045; Train Loss: 0.49748; Validation Loss: 0.50885\n",
      "Epoch: 046; Train Loss: 0.49247; Validation Loss: 0.50594\n",
      "Epoch: 047; Train Loss: 0.49190; Validation Loss: 0.49956\n",
      "Epoch: 048; Train Loss: 0.48803; Validation Loss: 0.49744\n",
      "Epoch: 049; Train Loss: 0.48521; Validation Loss: 0.49330\n",
      "Epoch: 050; Train Loss: 0.48281; Validation Loss: 0.49312\n",
      "Epoch: 051; Train Loss: 0.47679; Validation Loss: 0.49127\n",
      "Epoch: 052; Train Loss: 0.47584; Validation Loss: 0.48595\n",
      "Epoch: 053; Train Loss: 0.47504; Validation Loss: 0.48060\n",
      "Epoch: 054; Train Loss: 0.47072; Validation Loss: 0.48084\n",
      "Epoch: 055; Train Loss: 0.46801; Validation Loss: 0.47904\n",
      "Epoch: 056; Train Loss: 0.46791; Validation Loss: 0.47436\n",
      "Epoch: 057; Train Loss: 0.46236; Validation Loss: 0.47264\n",
      "Epoch: 058; Train Loss: 0.46030; Validation Loss: 0.46705\n",
      "Epoch: 059; Train Loss: 0.45806; Validation Loss: 0.46780\n",
      "Epoch: 060; Train Loss: 0.45368; Validation Loss: 0.46560\n",
      "Epoch: 061; Train Loss: 0.45287; Validation Loss: 0.46199\n",
      "Epoch: 062; Train Loss: 0.44900; Validation Loss: 0.46042\n",
      "Epoch: 063; Train Loss: 0.44603; Validation Loss: 0.45606\n",
      "Epoch: 064; Train Loss: 0.44486; Validation Loss: 0.45290\n",
      "Epoch: 065; Train Loss: 0.44170; Validation Loss: 0.45215\n",
      "Epoch: 066; Train Loss: 0.44198; Validation Loss: 0.45409\n",
      "Epoch: 067; Train Loss: 0.43643; Validation Loss: 0.44462\n",
      "Epoch: 068; Train Loss: 0.43561; Validation Loss: 0.44439\n",
      "Epoch: 069; Train Loss: 0.43259; Validation Loss: 0.44284\n",
      "Epoch: 070; Train Loss: 0.42989; Validation Loss: 0.43905\n",
      "Epoch: 071; Train Loss: 0.42617; Validation Loss: 0.43598\n",
      "Epoch: 072; Train Loss: 0.42219; Validation Loss: 0.43679\n",
      "Epoch: 073; Train Loss: 0.42095; Validation Loss: 0.43354\n",
      "Epoch: 074; Train Loss: 0.41774; Validation Loss: 0.43446\n",
      "Epoch: 075; Train Loss: 0.41612; Validation Loss: 0.42775\n",
      "Epoch: 076; Train Loss: 0.41336; Validation Loss: 0.42831\n",
      "Epoch: 077; Train Loss: 0.41398; Validation Loss: 0.42563\n",
      "Epoch: 078; Train Loss: 0.40924; Validation Loss: 0.42381\n",
      "Epoch: 079; Train Loss: 0.40911; Validation Loss: 0.41873\n",
      "Epoch: 080; Train Loss: 0.40388; Validation Loss: 0.41821\n",
      "Epoch: 081; Train Loss: 0.40420; Validation Loss: 0.41903\n",
      "Epoch: 082; Train Loss: 0.40322; Validation Loss: 0.41508\n",
      "Epoch: 083; Train Loss: 0.39893; Validation Loss: 0.40970\n",
      "Epoch: 084; Train Loss: 0.39787; Validation Loss: 0.41244\n",
      "Epoch: 085; Train Loss: 0.39403; Validation Loss: 0.41011\n",
      "Epoch: 086; Train Loss: 0.39293; Validation Loss: 0.40817\n",
      "Epoch: 087; Train Loss: 0.39119; Validation Loss: 0.40684\n",
      "Epoch: 088; Train Loss: 0.38784; Validation Loss: 0.40337\n",
      "Epoch: 089; Train Loss: 0.38991; Validation Loss: 0.40062\n",
      "Epoch: 090; Train Loss: 0.38559; Validation Loss: 0.39862\n",
      "Epoch: 091; Train Loss: 0.38447; Validation Loss: 0.39750\n",
      "Epoch: 092; Train Loss: 0.38262; Validation Loss: 0.39672\n",
      "Epoch: 093; Train Loss: 0.38260; Validation Loss: 0.39893\n",
      "Epoch: 094; Train Loss: 0.37595; Validation Loss: 0.39236\n",
      "Epoch: 095; Train Loss: 0.37558; Validation Loss: 0.39370\n",
      "Epoch: 096; Train Loss: 0.37271; Validation Loss: 0.38555\n",
      "Epoch: 097; Train Loss: 0.37202; Validation Loss: 0.38523\n",
      "Epoch: 098; Train Loss: 0.36685; Validation Loss: 0.38519\n",
      "Epoch: 099; Train Loss: 0.36789; Validation Loss: 0.38588\n",
      "Epoch: 100; Train Loss: 0.36547; Validation Loss: 0.38477\n",
      "Epoch: 101; Train Loss: 0.36243; Validation Loss: 0.38071\n",
      "Epoch: 102; Train Loss: 0.36114; Validation Loss: 0.37685\n",
      "Epoch: 103; Train Loss: 0.35832; Validation Loss: 0.38057\n",
      "Epoch: 104; Train Loss: 0.35676; Validation Loss: 0.37694\n",
      "Epoch: 105; Train Loss: 0.35634; Validation Loss: 0.37700\n",
      "Epoch: 106; Train Loss: 0.35370; Validation Loss: 0.37456\n",
      "Epoch: 107; Train Loss: 0.35256; Validation Loss: 0.37306\n",
      "Epoch: 108; Train Loss: 0.35078; Validation Loss: 0.36999\n",
      "Epoch: 109; Train Loss: 0.34998; Validation Loss: 0.36943\n",
      "Epoch: 110; Train Loss: 0.34678; Validation Loss: 0.37260\n",
      "Epoch: 111; Train Loss: 0.34481; Validation Loss: 0.36928\n",
      "Epoch: 112; Train Loss: 0.34319; Validation Loss: 0.36668\n",
      "Epoch: 113; Train Loss: 0.34162; Validation Loss: 0.36319\n",
      "Epoch: 114; Train Loss: 0.34164; Validation Loss: 0.36388\n",
      "Epoch: 115; Train Loss: 0.33639; Validation Loss: 0.36119\n",
      "Epoch: 116; Train Loss: 0.33517; Validation Loss: 0.35663\n",
      "Epoch: 117; Train Loss: 0.33375; Validation Loss: 0.35916\n",
      "Epoch: 118; Train Loss: 0.33324; Validation Loss: 0.35688\n",
      "Epoch: 119; Train Loss: 0.33341; Validation Loss: 0.35440\n",
      "Epoch: 120; Train Loss: 0.32959; Validation Loss: 0.35485\n",
      "Epoch: 121; Train Loss: 0.32742; Validation Loss: 0.35399\n",
      "Epoch: 122; Train Loss: 0.32735; Validation Loss: 0.35312\n",
      "Epoch: 123; Train Loss: 0.32424; Validation Loss: 0.34785\n",
      "Epoch: 124; Train Loss: 0.32233; Validation Loss: 0.35179\n",
      "Epoch: 125; Train Loss: 0.32170; Validation Loss: 0.34657\n",
      "Epoch: 126; Train Loss: 0.31864; Validation Loss: 0.34990\n",
      "Epoch: 127; Train Loss: 0.31616; Validation Loss: 0.34396\n",
      "Epoch: 128; Train Loss: 0.31771; Validation Loss: 0.34491\n",
      "Epoch: 129; Train Loss: 0.31392; Validation Loss: 0.34399\n",
      "Epoch: 130; Train Loss: 0.31344; Validation Loss: 0.33932\n",
      "Epoch: 131; Train Loss: 0.31117; Validation Loss: 0.34775\n",
      "Epoch: 132; Train Loss: 0.30816; Validation Loss: 0.34100\n",
      "Epoch: 133; Train Loss: 0.30905; Validation Loss: 0.33873\n",
      "Epoch: 134; Train Loss: 0.30780; Validation Loss: 0.33717\n",
      "Epoch: 135; Train Loss: 0.30542; Validation Loss: 0.33645\n",
      "Epoch: 136; Train Loss: 0.30354; Validation Loss: 0.33285\n",
      "Epoch: 137; Train Loss: 0.30241; Validation Loss: 0.33051\n",
      "Epoch: 138; Train Loss: 0.29999; Validation Loss: 0.33093\n",
      "Epoch: 139; Train Loss: 0.30086; Validation Loss: 0.33238\n",
      "Epoch: 140; Train Loss: 0.29727; Validation Loss: 0.33618\n",
      "Epoch: 141; Train Loss: 0.29584; Validation Loss: 0.33076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 142; Train Loss: 0.29547; Validation Loss: 0.32933\n",
      "Epoch: 143; Train Loss: 0.29259; Validation Loss: 0.32362\n",
      "Epoch: 144; Train Loss: 0.29244; Validation Loss: 0.32774\n",
      "Epoch: 145; Train Loss: 0.29067; Validation Loss: 0.32714\n",
      "Epoch: 146; Train Loss: 0.29098; Validation Loss: 0.32494\n",
      "Epoch: 147; Train Loss: 0.28719; Validation Loss: 0.32512\n",
      "Epoch: 148; Train Loss: 0.28859; Validation Loss: 0.31871\n",
      "Epoch: 149; Train Loss: 0.28727; Validation Loss: 0.31761\n",
      "Model Save! @ epoch 150, Loss 0.3213\n",
      "Epoch: 150; Train Loss: 0.28421; Validation Loss: 0.32133\n",
      "Model Save! @ epoch 151, Loss 0.3177\n",
      "Epoch: 151; Train Loss: 0.28049; Validation Loss: 0.31769\n",
      "Epoch: 152; Train Loss: 0.28214; Validation Loss: 0.32071\n",
      "Epoch: 153; Train Loss: 0.27966; Validation Loss: 0.32027\n",
      "Model Save! @ epoch 154, Loss 0.3150\n",
      "Epoch: 154; Train Loss: 0.27709; Validation Loss: 0.31503\n",
      "Model Save! @ epoch 155, Loss 0.3129\n",
      "Epoch: 155; Train Loss: 0.27642; Validation Loss: 0.31290\n",
      "Model Save! @ epoch 156, Loss 0.3127\n",
      "Epoch: 156; Train Loss: 0.27488; Validation Loss: 0.31275\n",
      "Epoch: 157; Train Loss: 0.27401; Validation Loss: 0.31580\n",
      "Epoch: 158; Train Loss: 0.27435; Validation Loss: 0.31280\n",
      "Model Save! @ epoch 159, Loss 0.3082\n",
      "Epoch: 159; Train Loss: 0.27022; Validation Loss: 0.30820\n",
      "Model Save! @ epoch 160, Loss 0.3077\n",
      "Epoch: 160; Train Loss: 0.27054; Validation Loss: 0.30775\n",
      "Epoch: 161; Train Loss: 0.27036; Validation Loss: 0.30876\n",
      "Epoch: 162; Train Loss: 0.26758; Validation Loss: 0.30785\n",
      "Epoch: 163; Train Loss: 0.26706; Validation Loss: 0.30892\n",
      "Model Save! @ epoch 164, Loss 0.3049\n",
      "Epoch: 164; Train Loss: 0.26409; Validation Loss: 0.30492\n",
      "Model Save! @ epoch 165, Loss 0.3045\n",
      "Epoch: 165; Train Loss: 0.26339; Validation Loss: 0.30452\n",
      "Model Save! @ epoch 166, Loss 0.3028\n",
      "Epoch: 166; Train Loss: 0.26331; Validation Loss: 0.30278\n",
      "Model Save! @ epoch 167, Loss 0.3010\n",
      "Epoch: 167; Train Loss: 0.26153; Validation Loss: 0.30102\n",
      "Epoch: 168; Train Loss: 0.25771; Validation Loss: 0.30285\n",
      "Model Save! @ epoch 169, Loss 0.2982\n",
      "Epoch: 169; Train Loss: 0.26049; Validation Loss: 0.29815\n",
      "Epoch: 170; Train Loss: 0.25779; Validation Loss: 0.30063\n",
      "Epoch: 171; Train Loss: 0.25490; Validation Loss: 0.30075\n",
      "Epoch: 172; Train Loss: 0.25559; Validation Loss: 0.30175\n",
      "Model Save! @ epoch 173, Loss 0.2974\n",
      "Epoch: 173; Train Loss: 0.25368; Validation Loss: 0.29742\n",
      "Model Save! @ epoch 174, Loss 0.2948\n",
      "Epoch: 174; Train Loss: 0.25294; Validation Loss: 0.29475\n",
      "Epoch: 175; Train Loss: 0.25334; Validation Loss: 0.29837\n",
      "Model Save! @ epoch 176, Loss 0.2937\n",
      "Epoch: 176; Train Loss: 0.25037; Validation Loss: 0.29366\n",
      "Epoch: 177; Train Loss: 0.24959; Validation Loss: 0.29538\n",
      "Epoch: 178; Train Loss: 0.24905; Validation Loss: 0.29497\n",
      "Model Save! @ epoch 179, Loss 0.2929\n",
      "Epoch: 179; Train Loss: 0.24838; Validation Loss: 0.29290\n",
      "Model Save! @ epoch 180, Loss 0.2909\n",
      "Epoch: 180; Train Loss: 0.24740; Validation Loss: 0.29087\n",
      "Epoch: 181; Train Loss: 0.24636; Validation Loss: 0.29116\n",
      "Epoch: 182; Train Loss: 0.24444; Validation Loss: 0.29121\n",
      "Epoch: 183; Train Loss: 0.24089; Validation Loss: 0.29187\n",
      "Model Save! @ epoch 184, Loss 0.2889\n",
      "Epoch: 184; Train Loss: 0.24128; Validation Loss: 0.28887\n",
      "Epoch: 185; Train Loss: 0.24383; Validation Loss: 0.29438\n",
      "Epoch: 186; Train Loss: 0.24075; Validation Loss: 0.29004\n",
      "Epoch: 187; Train Loss: 0.23981; Validation Loss: 0.28902\n",
      "Model Save! @ epoch 188, Loss 0.2873\n",
      "Epoch: 188; Train Loss: 0.23824; Validation Loss: 0.28725\n",
      "Model Save! @ epoch 189, Loss 0.2839\n",
      "Epoch: 189; Train Loss: 0.23627; Validation Loss: 0.28390\n",
      "Epoch: 190; Train Loss: 0.23661; Validation Loss: 0.28664\n",
      "Model Save! @ epoch 191, Loss 0.2812\n",
      "Epoch: 191; Train Loss: 0.23352; Validation Loss: 0.28123\n",
      "Epoch: 192; Train Loss: 0.23343; Validation Loss: 0.28812\n",
      "Epoch: 193; Train Loss: 0.23470; Validation Loss: 0.28377\n",
      "Epoch: 194; Train Loss: 0.23046; Validation Loss: 0.28611\n",
      "Epoch: 195; Train Loss: 0.22994; Validation Loss: 0.28213\n",
      "Model Save! @ epoch 196, Loss 0.2780\n",
      "Epoch: 196; Train Loss: 0.22848; Validation Loss: 0.27801\n",
      "Epoch: 197; Train Loss: 0.22923; Validation Loss: 0.28188\n",
      "Model Save! @ epoch 198, Loss 0.2760\n",
      "Epoch: 198; Train Loss: 0.22841; Validation Loss: 0.27604\n",
      "Epoch: 199; Train Loss: 0.22684; Validation Loss: 0.28018\n",
      "Epoch: 200; Train Loss: 0.22576; Validation Loss: 0.28043\n",
      "Epoch: 201; Train Loss: 0.22518; Validation Loss: 0.27860\n",
      "Epoch: 202; Train Loss: 0.22414; Validation Loss: 0.27745\n",
      "Epoch: 203; Train Loss: 0.22293; Validation Loss: 0.27877\n",
      "Epoch: 204; Train Loss: 0.22102; Validation Loss: 0.27830\n",
      "Model Save! @ epoch 205, Loss 0.2753\n",
      "Epoch: 205; Train Loss: 0.22010; Validation Loss: 0.27534\n",
      "Epoch: 206; Train Loss: 0.22210; Validation Loss: 0.27690\n",
      "Epoch: 207; Train Loss: 0.21812; Validation Loss: 0.27627\n",
      "Model Save! @ epoch 208, Loss 0.2730\n",
      "Epoch: 208; Train Loss: 0.21713; Validation Loss: 0.27303\n",
      "Model Save! @ epoch 209, Loss 0.2729\n",
      "Epoch: 209; Train Loss: 0.21947; Validation Loss: 0.27292\n",
      "Model Save! @ epoch 210, Loss 0.2722\n",
      "Epoch: 210; Train Loss: 0.21605; Validation Loss: 0.27221\n",
      "Model Save! @ epoch 211, Loss 0.2709\n",
      "Epoch: 211; Train Loss: 0.21420; Validation Loss: 0.27085\n",
      "Epoch: 212; Train Loss: 0.21324; Validation Loss: 0.27846\n",
      "Epoch: 213; Train Loss: 0.21546; Validation Loss: 0.27464\n",
      "Epoch: 214; Train Loss: 0.21411; Validation Loss: 0.27817\n",
      "Epoch: 215; Train Loss: 0.20901; Validation Loss: 0.27680\n",
      "Epoch: 216; Train Loss: 0.21270; Validation Loss: 0.27088\n",
      "Model Save! @ epoch 217, Loss 0.2704\n",
      "Epoch: 217; Train Loss: 0.21207; Validation Loss: 0.27045\n",
      "Model Save! @ epoch 218, Loss 0.2703\n",
      "Epoch: 218; Train Loss: 0.21093; Validation Loss: 0.27029\n",
      "Epoch: 219; Train Loss: 0.21028; Validation Loss: 0.27172\n",
      "Model Save! @ epoch 220, Loss 0.2668\n",
      "Epoch: 220; Train Loss: 0.20890; Validation Loss: 0.26682\n",
      "Epoch: 221; Train Loss: 0.20787; Validation Loss: 0.27315\n",
      "Model Save! @ epoch 222, Loss 0.2664\n",
      "Epoch: 222; Train Loss: 0.20902; Validation Loss: 0.26642\n",
      "Epoch: 223; Train Loss: 0.20721; Validation Loss: 0.26815\n",
      "Model Save! @ epoch 224, Loss 0.2651\n",
      "Epoch: 224; Train Loss: 0.20446; Validation Loss: 0.26514\n",
      "Model Save! @ epoch 225, Loss 0.2632\n",
      "Epoch: 225; Train Loss: 0.20356; Validation Loss: 0.26324\n",
      "Epoch: 226; Train Loss: 0.20296; Validation Loss: 0.26889\n",
      "Model Save! @ epoch 227, Loss 0.2624\n",
      "Epoch: 227; Train Loss: 0.20020; Validation Loss: 0.26235\n",
      "Epoch: 228; Train Loss: 0.20063; Validation Loss: 0.26712\n",
      "Epoch: 229; Train Loss: 0.19994; Validation Loss: 0.26915\n",
      "Epoch: 230; Train Loss: 0.19793; Validation Loss: 0.26694\n",
      "Epoch: 231; Train Loss: 0.19651; Validation Loss: 0.26789\n",
      "Epoch: 232; Train Loss: 0.19635; Validation Loss: 0.26342\n",
      "Model Save! @ epoch 233, Loss 0.2590\n",
      "Epoch: 233; Train Loss: 0.19380; Validation Loss: 0.25896\n",
      "Epoch: 234; Train Loss: 0.19808; Validation Loss: 0.26176\n",
      "Epoch: 235; Train Loss: 0.19796; Validation Loss: 0.26252\n",
      "Epoch: 236; Train Loss: 0.19328; Validation Loss: 0.26174\n",
      "Epoch: 237; Train Loss: 0.19277; Validation Loss: 0.26323\n",
      "Epoch: 238; Train Loss: 0.19355; Validation Loss: 0.26216\n",
      "Model Save! @ epoch 239, Loss 0.2575\n",
      "Epoch: 239; Train Loss: 0.19268; Validation Loss: 0.25748\n",
      "Epoch: 240; Train Loss: 0.19044; Validation Loss: 0.26100\n",
      "Epoch: 241; Train Loss: 0.19101; Validation Loss: 0.25762\n",
      "Epoch: 242; Train Loss: 0.18971; Validation Loss: 0.26388\n",
      "Epoch: 243; Train Loss: 0.19077; Validation Loss: 0.25781\n",
      "Epoch: 244; Train Loss: 0.18994; Validation Loss: 0.26216\n",
      "Model Save! @ epoch 245, Loss 0.2553\n",
      "Epoch: 245; Train Loss: 0.18877; Validation Loss: 0.25530\n",
      "Epoch: 246; Train Loss: 0.18553; Validation Loss: 0.25992\n",
      "Epoch: 247; Train Loss: 0.18599; Validation Loss: 0.25733\n",
      "Epoch: 248; Train Loss: 0.18458; Validation Loss: 0.25716\n",
      "Epoch: 249; Train Loss: 0.18379; Validation Loss: 0.25819\n",
      "Epoch: 250; Train Loss: 0.18314; Validation Loss: 0.25866\n",
      "Epoch: 251; Train Loss: 0.18142; Validation Loss: 0.25748\n",
      "Epoch: 252; Train Loss: 0.18086; Validation Loss: 0.25799\n",
      "Epoch: 253; Train Loss: 0.18263; Validation Loss: 0.25605\n",
      "Epoch: 254; Train Loss: 0.18417; Validation Loss: 0.25585\n",
      "Epoch: 255; Train Loss: 0.18096; Validation Loss: 0.25603\n",
      "Epoch: 256; Train Loss: 0.18181; Validation Loss: 0.25655\n",
      "Model Save! @ epoch 257, Loss 0.2522\n",
      "Epoch: 257; Train Loss: 0.18023; Validation Loss: 0.25216\n",
      "Epoch: 258; Train Loss: 0.17942; Validation Loss: 0.25578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Save! @ epoch 259, Loss 0.2515\n",
      "Epoch: 259; Train Loss: 0.17701; Validation Loss: 0.25154\n",
      "Epoch: 260; Train Loss: 0.17638; Validation Loss: 0.25638\n",
      "Epoch: 261; Train Loss: 0.17731; Validation Loss: 0.25766\n",
      "Epoch: 262; Train Loss: 0.17426; Validation Loss: 0.25390\n",
      "Epoch: 263; Train Loss: 0.17656; Validation Loss: 0.25260\n",
      "Epoch: 264; Train Loss: 0.17578; Validation Loss: 0.25495\n",
      "Epoch: 265; Train Loss: 0.17482; Validation Loss: 0.25554\n",
      "Epoch: 266; Train Loss: 0.17152; Validation Loss: 0.25823\n",
      "Model Save! @ epoch 267, Loss 0.2498\n",
      "Epoch: 267; Train Loss: 0.17301; Validation Loss: 0.24979\n",
      "Epoch: 268; Train Loss: 0.17169; Validation Loss: 0.25290\n",
      "Epoch: 269; Train Loss: 0.17181; Validation Loss: 0.25203\n",
      "Model Save! @ epoch 270, Loss 0.2495\n",
      "Epoch: 270; Train Loss: 0.17015; Validation Loss: 0.24946\n",
      "Epoch: 271; Train Loss: 0.17063; Validation Loss: 0.25304\n",
      "Epoch: 272; Train Loss: 0.17098; Validation Loss: 0.25070\n",
      "Epoch: 273; Train Loss: 0.16938; Validation Loss: 0.25386\n",
      "Epoch: 274; Train Loss: 0.16777; Validation Loss: 0.25363\n",
      "Epoch: 275; Train Loss: 0.16845; Validation Loss: 0.25161\n",
      "Model Save! @ epoch 276, Loss 0.2475\n",
      "Epoch: 276; Train Loss: 0.16783; Validation Loss: 0.24750\n",
      "Epoch: 277; Train Loss: 0.16646; Validation Loss: 0.24759\n",
      "Epoch: 278; Train Loss: 0.16646; Validation Loss: 0.25089\n",
      "Epoch: 279; Train Loss: 0.16470; Validation Loss: 0.25362\n",
      "Epoch: 280; Train Loss: 0.16387; Validation Loss: 0.25016\n",
      "Epoch: 281; Train Loss: 0.16419; Validation Loss: 0.25019\n",
      "Epoch: 282; Train Loss: 0.16362; Validation Loss: 0.24968\n",
      "Epoch: 283; Train Loss: 0.16363; Validation Loss: 0.25198\n",
      "Model Save! @ epoch 284, Loss 0.2459\n",
      "Epoch: 284; Train Loss: 0.16202; Validation Loss: 0.24586\n",
      "Epoch: 285; Train Loss: 0.16338; Validation Loss: 0.25168\n",
      "Model Save! @ epoch 286, Loss 0.2449\n",
      "Epoch: 286; Train Loss: 0.16186; Validation Loss: 0.24488\n",
      "Epoch: 287; Train Loss: 0.16037; Validation Loss: 0.24747\n",
      "Epoch: 288; Train Loss: 0.15941; Validation Loss: 0.24801\n",
      "Epoch: 289; Train Loss: 0.15987; Validation Loss: 0.24644\n",
      "Epoch: 290; Train Loss: 0.15859; Validation Loss: 0.25348\n",
      "Epoch: 291; Train Loss: 0.15748; Validation Loss: 0.24667\n",
      "Epoch: 292; Train Loss: 0.15665; Validation Loss: 0.24635\n",
      "Epoch: 293; Train Loss: 0.15836; Validation Loss: 0.24883\n",
      "Epoch: 294; Train Loss: 0.15670; Validation Loss: 0.24632\n",
      "Epoch: 295; Train Loss: 0.15457; Validation Loss: 0.25025\n",
      "Epoch: 296; Train Loss: 0.15568; Validation Loss: 0.24950\n",
      "Epoch: 297; Train Loss: 0.15577; Validation Loss: 0.24673\n",
      "Epoch: 298; Train Loss: 0.15509; Validation Loss: 0.24699\n",
      "Model Save! @ epoch 299, Loss 0.2425\n",
      "Epoch: 299; Train Loss: 0.15784; Validation Loss: 0.24252\n",
      "CPU times: user 36min 7s, sys: 3.31 s, total: 36min 10s\n",
      "Wall time: 36min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_process(model, device, train_loader, criterion, optimizer, epoch)\n",
    "    verify_process(model, device, valid_loader, criterion, epoch)\n",
    "    if epoch % log_interval == 0:\n",
    "        print(\n",
    "            'Epoch: {:0>3d}; Train Loss: {:.5f}; Validation Loss: {:.5f}'.format(\n",
    "                epoch, \n",
    "                train_results[-1][1], \n",
    "                verify_results[-1][1]\n",
    "            )\n",
    "        )\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.24252387881278992"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss = [a for e, a in train_results]\n",
    "train_eps = [e for e, a in train_results]\n",
    "valid_loss = [a for e, a in verify_results]\n",
    "valid_eps = [e for e, a in verify_results]\n",
    "plt.plot(train_eps, train_loss, 'b-')\n",
    "plt.plot(valid_eps, valid_loss, 'r.')\n",
    "plt.show()\n",
    "min(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv1d(1, 2, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "  (conv1_bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  (conv2): Conv1d(2, 4, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "  (conv2_bn): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (conv3): Conv1d(4, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "  (conv3_bn): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout3): Dropout(p=0.2, inplace=False)\n",
       "  (fc1): Linear(in_features=3400, out_features=1000, bias=True)\n",
       "  (fc1_bn): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout_fc1): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=1000, out_features=250, bias=True)\n",
       "  (fc2_bn): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout_fc2): Dropout(p=0.2, inplace=False)\n",
       "  (fc3): Linear(in_features=250, out_features=64, bias=True)\n",
       "  (fc3_bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout_fc3): Dropout(p=0.2, inplace=False)\n",
       "  (fc4): Linear(in_features=64, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = torch.load(model_path)\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG00; t_acc = 88.66% (O); f_acc = 93.71% (O)\n",
      "FG01; t_acc = 86.49% (O); f_acc = 87.25% (O)\n",
      "FG02; t_acc = 97.14% (O); f_acc = 93.21% (O)\n",
      "FG03; t_acc = 87.50% (O); f_acc = 83.06% (O)\n",
      "FG04; t_acc = 89.29% (O); f_acc = 80.06% (O)\n",
      "FG05; t_acc = 91.84% (O); f_acc = 94.88% (O)\n",
      "FG06; t_acc = 89.19% (O); f_acc = 90.23% (O)\n",
      "FG07; t_acc = 84.38% (O); f_acc = 90.58% (O)\n",
      "FG08; t_acc = 81.08% (O); f_acc = 76.72% (O)\n",
      "FG09; t_acc = 89.66% (O); f_acc = 96.45% (O)\n",
      "FG10; t_acc = 96.43% (O); f_acc = 93.76% (O)\n",
      "FG11; t_acc = 86.67% (O); f_acc = 97.16% (O)\n",
      "FG12; t_acc = 92.31% (O); f_acc = 94.77% (O)\n",
      "FG13; t_acc = 96.15% (O); f_acc = 97.31% (O)\n",
      "FG14; t_acc = 92.59% (O); f_acc = 95.75% (O)\n",
      "FG15; t_acc = 83.33% (O); f_acc = 90.47% (O)\n",
      "FG16; t_acc = 88.00% (O); f_acc = 88.56% (O)\n",
      "FG17; t_acc = 81.48% (O); f_acc = 95.61% (O)\n",
      "FG18; t_acc = 87.50% (O); f_acc = 91.26% (O)\n",
      "FG19; t_acc = 80.00% (O); f_acc = 99.72% (O)\n",
      "FG20; t_acc = 94.44% (O); f_acc = 95.24% (O)\n",
      "FG21; t_acc = 93.75% (O); f_acc = 96.09% (O)\n",
      "FG22; t_acc = 78.57% (O); f_acc = 89.29% (O)\n",
      "FG23; t_acc = 100.00% (O); f_acc = 95.14% (O)\n",
      "FG24; t_acc = 82.35% (O); f_acc = 96.51% (O)\n",
      "FG25; t_acc = 75.00% (O); f_acc = 98.89% (O)\n",
      "FG26; t_acc = 91.67% (O); f_acc = 96.81% (O)\n",
      "FG27; t_acc = 91.67% (O); f_acc = 94.04% (O)\n",
      "FG28; t_acc = 84.62% (O); f_acc = 93.89% (O)\n",
      "FG29; t_acc = 72.73% (X); f_acc = 98.20% (O)\n",
      "FG30; t_acc = 90.00% (O); f_acc = 96.96% (O)\n",
      "FG31; t_acc = 63.64% (X); f_acc = 93.07% (O)\n",
      "FG32; t_acc = 90.91% (O); f_acc = 94.60% (O)\n",
      "FG33; t_acc = 90.00% (O); f_acc = 94.19% (O)\n",
      "FG34; t_acc = 87.50% (O); f_acc = 96.69% (O)\n",
      "FG35; t_acc = 90.00% (O); f_acc = 97.23% (O)\n",
      "FG36; t_acc = 87.50% (O); f_acc = 97.93% (O)\n",
      "FG37; t_acc = 87.50% (O); f_acc = 99.72% (O)\n",
      "FG38; t_acc = 100.00% (O); f_acc = 97.38% (O)\n",
      "FG39; t_acc = 100.00% (O); f_acc = 92.27% (O)\n",
      "FG40; t_acc = 90.00% (O); f_acc = 98.76% (O)\n",
      "FG41; t_acc = 88.89% (O); f_acc = 96.27% (O)\n",
      "FG42; t_acc = 44.44% (X); f_acc = 93.78% (O)\n",
      "FG43; t_acc = 87.50% (O); f_acc = 96.55% (O)\n",
      "FG44; t_acc = 100.00% (O); f_acc = 99.17% (O)\n",
      "FG45; t_acc = 77.78% (O); f_acc = 99.31% (O)\n",
      "FG46; t_acc = 87.50% (O); f_acc = 96.83% (O)\n",
      "FG47; t_acc = 100.00% (O); f_acc = 98.08% (O)\n",
      "FG48; t_acc = 100.00% (O); f_acc = 97.79% (O)\n",
      "FG49; t_acc = 75.00% (O); f_acc = 97.38% (O)\n",
      "number of useful FGs in valid sets (t_acc and f_acc are more than 75%) =  47\n"
     ]
    }
   ],
   "source": [
    "ans = verify_process(loaded_model, device, valid_loader, criterion, 0)\n",
    "\n",
    "yhs, ys = ans[0]\n",
    "\n",
    "yhs = yhs.cpu().numpy()\n",
    "ys = ys.cpu().numpy()\n",
    "\n",
    "fail_valid_idxs = []\n",
    "\n",
    "count_75 = 0\n",
    "for nc in range(num_classes):\n",
    "    t_total, t_correct, t_wrong = 0, 0, 0\n",
    "    f_total, f_correct, f_wrong = 0, 0, 0\n",
    "\n",
    "    for idx in range(len(ys)):\n",
    "        yh, y = yhs[idx][nc], ys[idx][nc]\n",
    "        if y == 1:\n",
    "            t_total += 1\n",
    "            if yh >= 0.5:\n",
    "                t_correct += 1\n",
    "            else:\n",
    "                t_wrong += 1\n",
    "        else:\n",
    "            f_total += 1\n",
    "            if yh < 0.5:\n",
    "                f_correct += 1\n",
    "            else:\n",
    "                f_wrong += 1\n",
    "    if not t_total:\n",
    "        print('>>>>>    {} - - - - -'.format(nc))\n",
    "        continue\n",
    "    t_acc = t_correct * 100.0 / t_total\n",
    "    f_acc = f_correct * 100.0 / f_total\n",
    "    print('FG{:0>2d}; t_acc = {:.2f}% ({}); f_acc = {:.2f}% ({})'.format(\n",
    "        nc, t_acc, 'O' if t_acc >= 75.0 else 'X', f_acc, 'O' if f_acc >= 75.0 else 'X'))\n",
    "    if t_acc >= 75.0 and f_acc >= 75.0:\n",
    "        count_75 += 1\n",
    "    else:\n",
    "        fail_valid_idxs.append(nc)\n",
    "print('number of useful FGs in valid sets (t_acc and f_acc are more than 75%) = ', count_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG00; t_acc = 95.65% (O); f_acc = 95.36% (O)\n",
      "FG01; t_acc = 82.72% (O); f_acc = 87.39% (O)\n",
      "FG02; t_acc = 97.10% (O); f_acc = 93.28% (O)\n",
      "FG03; t_acc = 88.57% (O); f_acc = 79.82% (O)\n",
      "FG04; t_acc = 91.07% (O); f_acc = 78.48% (O)\n",
      "FG05; t_acc = 88.37% (O); f_acc = 93.39% (O)\n",
      "FG06; t_acc = 90.00% (O); f_acc = 89.84% (O)\n",
      "FG07; t_acc = 96.88% (O); f_acc = 90.10% (O)\n",
      "FG08; t_acc = 91.43% (O); f_acc = 75.57% (O)\n",
      "FG09; t_acc = 93.55% (O); f_acc = 95.76% (O)\n",
      "FG10; t_acc = 93.55% (O); f_acc = 93.79% (O)\n",
      "FG11; t_acc = 86.21% (O); f_acc = 95.63% (O)\n",
      "FG12; t_acc = 96.43% (O); f_acc = 94.09% (O)\n",
      "FG13; t_acc = 84.85% (O); f_acc = 97.88% (O)\n",
      "FG14; t_acc = 100.00% (O); f_acc = 95.93% (O)\n",
      "FG15; t_acc = 96.67% (O); f_acc = 91.96% (O)\n",
      "FG16; t_acc = 80.00% (O); f_acc = 86.83% (O)\n",
      "FG17; t_acc = 80.00% (O); f_acc = 96.22% (O)\n",
      "FG18; t_acc = 77.27% (O); f_acc = 91.35% (O)\n",
      "FG19; t_acc = 84.21% (O); f_acc = 99.58% (O)\n",
      "FG20; t_acc = 88.24% (O); f_acc = 95.43% (O)\n",
      "FG21; t_acc = 72.22% (X); f_acc = 95.28% (O)\n",
      "FG22; t_acc = 57.14% (X); f_acc = 89.66% (O)\n",
      "FG23; t_acc = 78.57% (O); f_acc = 95.59% (O)\n",
      "FG24; t_acc = 92.31% (O); f_acc = 96.14% (O)\n",
      "FG25; t_acc = 100.00% (O); f_acc = 98.90% (O)\n",
      "FG26; t_acc = 92.31% (O); f_acc = 96.42% (O)\n",
      "FG27; t_acc = 92.31% (O); f_acc = 93.80% (O)\n",
      "FG28; t_acc = 100.00% (O); f_acc = 94.91% (O)\n",
      "FG29; t_acc = 90.91% (O); f_acc = 97.39% (O)\n",
      "FG30; t_acc = 90.91% (O); f_acc = 96.57% (O)\n",
      "FG31; t_acc = 63.64% (X); f_acc = 92.45% (O)\n",
      "FG32; t_acc = 100.00% (O); f_acc = 95.87% (O)\n",
      "FG33; t_acc = 90.00% (O); f_acc = 94.51% (O)\n",
      "FG34; t_acc = 71.43% (X); f_acc = 97.40% (O)\n",
      "FG35; t_acc = 80.00% (O); f_acc = 97.81% (O)\n",
      "FG36; t_acc = 88.89% (O); f_acc = 98.49% (O)\n",
      "FG37; t_acc = 77.78% (O); f_acc = 99.73% (O)\n",
      "FG38; t_acc = 60.00% (X); f_acc = 95.75% (O)\n",
      "FG39; t_acc = 88.89% (O); f_acc = 92.33% (O)\n",
      "FG40; t_acc = 100.00% (O); f_acc = 98.77% (O)\n",
      "FG41; t_acc = 100.00% (O); f_acc = 96.85% (O)\n",
      "FG42; t_acc = 75.00% (O); f_acc = 92.89% (O)\n",
      "FG43; t_acc = 87.50% (O); f_acc = 97.26% (O)\n",
      "FG44; t_acc = 85.71% (O); f_acc = 98.77% (O)\n",
      "FG45; t_acc = 55.56% (X); f_acc = 99.45% (O)\n",
      "FG46; t_acc = 100.00% (O); f_acc = 98.08% (O)\n",
      "FG47; t_acc = 50.00% (X); f_acc = 97.28% (O)\n",
      "FG48; t_acc = 100.00% (O); f_acc = 98.91% (O)\n",
      "FG49; t_acc = 87.50% (O); f_acc = 96.03% (O)\n",
      "number of useful FGs in test sets (t_acc and f_acc are more than 75%) =  43\n"
     ]
    }
   ],
   "source": [
    "ans = verify_process(loaded_model, device, test_loader, criterion, 0)\n",
    "\n",
    "yhs, ys = ans[0]\n",
    "\n",
    "yhs = yhs.cpu().numpy()\n",
    "ys = ys.cpu().numpy()\n",
    "\n",
    "fail_test_idxs = []\n",
    "\n",
    "count_75 = 0\n",
    "for nc in range(num_classes):\n",
    "    t_total, t_correct, t_wrong = 0, 0, 0\n",
    "    f_total, f_correct, f_wrong = 0, 0, 0\n",
    "\n",
    "    for idx in range(len(ys)):\n",
    "        yh, y = yhs[idx][nc], ys[idx][nc]\n",
    "        if y == 1:\n",
    "            t_total += 1\n",
    "            if yh >= 0.5:\n",
    "                t_correct += 1\n",
    "            else:\n",
    "                t_wrong += 1\n",
    "        else:\n",
    "            f_total += 1\n",
    "            if yh < 0.5:\n",
    "                f_correct += 1\n",
    "            else:\n",
    "                f_wrong += 1\n",
    "    if not t_total:\n",
    "        print('>>>>>    {} - - - - -'.format(nc))\n",
    "        continue\n",
    "    t_acc = t_correct * 100.0 / t_total\n",
    "    f_acc = f_correct * 100.0 / f_total\n",
    "    print('FG{:0>2d}; t_acc = {:.2f}% ({}); f_acc = {:.2f}% ({})'.format(\n",
    "        nc, t_acc, 'O' if t_acc >= 75.0 else 'X', f_acc, 'O' if f_acc >= 75.0 else 'X'))\n",
    "    if t_acc >= 75.0 and f_acc >= 75.0:\n",
    "        count_75 += 1\n",
    "    else:\n",
    "        fail_test_idxs.append(nc)\n",
    "print('number of useful FGs in test sets (t_acc and f_acc are more than 75%) = ', count_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29, 31, 42]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fail_valid_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 22, 31, 34, 38, 45, 47]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fail_test_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(fail_valid_idxs + fail_test_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{21, 22, 29, 31, 34, 38, 42, 45, 47}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(fail_valid_idxs + fail_test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-ir-01",
   "language": "python",
   "name": "deep-ir-01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
